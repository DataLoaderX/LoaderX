{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单字特征的解读\n",
    "\n",
    "本教程讨论 HWDB1.0\\~1.1 以及 OLHWDB1.0\\~1.1，下载到个人电脑的同一目录下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CASI 数据集所在根目录\n",
    "root = 'D:/datasets/OCR/CASIA/data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入本教程需要使用的包："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "from pathlib import Path\n",
    "from zipfile import ZipFile\n",
    "import numpy as np\n",
    "import tables as tb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Path` 更加友好的管理文件的路径："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HWDB1.0trn.zip',\n",
       " 'HWDB1.0trn_gnt.zip',\n",
       " 'HWDB1.0tst.zip',\n",
       " 'HWDB1.0tst_gnt.zip',\n",
       " 'HWDB1.1trn.zip',\n",
       " 'HWDB1.1trn_gnt.zip',\n",
       " 'HWDB1.1tst.zip',\n",
       " 'HWDB1.1tst_gnt.zip',\n",
       " 'OLHWDB1.0test_pot.zip',\n",
       " 'OLHWDB1.0train_pot.zip',\n",
       " 'OLHWDB1.0trn.zip',\n",
       " 'OLHWDB1.0tst.zip',\n",
       " 'OLHWDB1.1trn.zip',\n",
       " 'OLHWDB1.1trn_pot.zip',\n",
       " 'OLHWDB1.1tst.zip',\n",
       " 'OLHWDB1.1tst_pot.zip']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = Path(root)\n",
    "# 查看 root 的全部文件\n",
    "[name.parts[-1] for name in root.iterdir()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个单字的特征均以 `.mpf` 形式保存手工特征，可以看出上述文件均为压缩包，下面使用 `zipfile` 对压缩文件进行解读："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HWDB1.0trn/001.mpf',\n",
       " 'HWDB1.0trn/002.mpf',\n",
       " 'HWDB1.0trn/003.mpf',\n",
       " 'HWDB1.0trn/004.mpf']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = ZipFile(list(root.glob('**/HWDB1.0trn.zip'))[0])\n",
    "z.namelist()[1:5] # 查看前4个人写的 MPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入 MPF 的解码器：MPFDecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../loader\")\n",
    "from casia.feature import MPFDecoder, zipfile2bunch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 MPF 转换为 bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zip_name = list(root.glob('**/HWDB1.0trn.zip'))[0]\n",
    "mb = zipfile2bunch(zip_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 bunch 转换为 JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.dataset import bunch2json, json2bunch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('data')\n",
    "if not data_dir.exists(): # 如果不存在\n",
    "    data_dir.mkdir() # 创建目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.14 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "json_path = 'data/features.json'\n",
    "bunch2json(mb, json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 再次载入数据\n",
    "mpf_bunch = json2bunch(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将 bunch 转换为 HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bunch2hdf(bunch, save_path):\n",
    "    '''将 bunch 转换为 HDF5'''\n",
    "    filters = tb.Filters(complevel=7, shuffle=False)  # 过滤信息，用于压缩文件\n",
    "    h = tb.open_file(save_path, 'w', filters=filters, title='Xinet\\'s dataset')\n",
    "    for name in bunch:  # 生成数据集\"头\"\n",
    "        _name = name.replace('/', '__')\n",
    "        _name = _name.replace('.', '_')\n",
    "        h.create_group('/', name=_name, filters=filters)\n",
    "        h.create_array(f\"/{_name}\", 'text',\n",
    "                       bunch[name]['text'].encode())\n",
    "        features = bunch[name]['dataset']\n",
    "        h.create_array(f\"/{_name}\", 'labels',\n",
    "                       \" \".join([l for l in features.index]).encode())\n",
    "        h.create_array(f\"/{_name}\", 'features', features.values)\n",
    "    h.close()  # 防止资源泄露"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.29 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hdf_path = 'data/features.h5'\n",
    "bunch2hdf(mpf_bunch, hdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 996 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "h5 = tb.open_file(hdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3728, 512)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取某个 mpf 的特征矩阵的 shape\n",
    "h5.root.HWDB1_0trn__001_mpf.features[:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Character features extracted from grayscale images. #ftrtype=ncg, #norm=ldi, #aspect=4, #dirn=8, #zone=8, #zstep=8, #fstep=8, $deslant=0, $smooth=0, $nmdir=0, $multisc=0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取某个 mpf 的特征介绍\n",
    "h5.root.HWDB1_0trn__001_mpf.text.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['扼', '遏', '鄂', ..., '娥', '恶', '厄'], dtype='<U1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取某个 mpf 的标签信息\n",
    "labels = h5.root.HWDB1_0trn__001_mpf.labels.read().decode()\n",
    "labels = np.array(labels.split(' '))\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试 JSON 与 HDF5 的文件大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Python 对象占用空间大小为 9.32 kB, 文件大小为 0.65487944 G\n",
      "HDF5 Python 对象占用空间大小为 80 B, 文件大小为 0.644292324 G\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from sys import getsizeof\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"JSON Python 对象占用空间大小为 {getsizeof(mpf_bunch)/1e3} kB, 文件大小为 {os.path.getsize(json_path)/1e9} G\")\n",
    "print(\n",
    "    f\"HDF5 Python 对象占用空间大小为 {getsizeof(h5)} B, 文件大小为 {os.path.getsize(hdf_path)/1e9} G\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h5.close()  # 关闭"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 打包多个 zip 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{WindowsPath('D:/datasets/OCR/CASIA/data/HWDB1.0trn.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/HWDB1.0tst.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/HWDB1.1trn.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/HWDB1.1tst.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/OLHWDB1.0trn.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/OLHWDB1.0tst.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/OLHWDB1.1trn.zip'),\n",
       " WindowsPath('D:/datasets/OCR/CASIA/data/OLHWDB1.1tst.zip')}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zip_gnt_names = set(root.glob('*gnt*.zip')) # GNT 名称列表\n",
    "zip_pot_names = set(root.glob('*pot*.zip')) # POT名称列表\n",
    "# MPF 名称列表\n",
    "zip_mpf_names = set(root.iterdir()) - zip_pot_names - zip_gnt_names\n",
    "zip_mpf_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpf_bunch = {}\n",
    "for mpf_name in zip_mpf_names:\n",
    "    mpf_bunch.update(zipfile2bunch(mpf_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存为 JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "json_path = 'data/features.json'\n",
    "bunch2json(mpf_bunch, json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存为 HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 12.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "hdf_path = 'data/features.h5'\n",
    "bunch2hdf(mpf_bunch, hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入 JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "mpf_bunch = json2bunch(json_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入 HDF5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "h5 = tb.open_file(hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 再次测试文件大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON Python 对象占用空间大小为 73.824 kB, 文件大小为 2.82091995 G\n",
      "HDF5 Python 对象占用空间大小为 80 B, 文件大小为 2.775284889 G\n"
     ]
    }
   ],
   "source": [
    "from sys import getsizeof\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"JSON Python 对象占用空间大小为 {getsizeof(mpf_bunch)/1e3} kB, 文件大小为 {Path(json_path).stat().st_size/1e9} G\")\n",
    "print(\n",
    "    f\"HDF5 Python 对象占用空间大小为 {getsizeof(h5)} B, 文件大小为 {Path(hdf_path).stat().st_size/1e9} G\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上述的展示可以看出 HDF5 优于 JSON 与 ZipFile，所以下面仅仅考虑 HDF5 文件。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 解析 features.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2775284889"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5.get_filesize() # 获取文件大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = h5.list_nodes('/')  # 列出所有 MPF 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/HWDB1_0trn__001_mpf (Group) ''\n",
       "  children := ['features' (Array), 'labels' (Array), 'text' (Array)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodes[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1440"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nodes)  # 统计 MPF 个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = h5.iter_nodes('/') # 所有 MPF 数据以迭代器的方式使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/HWDB1_0trn__001_mpf (Group) ''\n",
       "  children := ['features' (Array), 'labels' (Array), 'text' (Array)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(data_iter) # 取出一个 MPF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取 MPF 的特征矩阵与标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/HWDB1_0trn__007_mpf (Group) ''\n",
       "  children := ['features' (Array), 'labels' (Array), 'text' (Array)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mpf_name = 'HWDB1_0trn__007_mpf'\n",
    "# 依据 MPF 的名称获取 MPF\n",
    "mpf = h5.get_node('/', mpf_name)\n",
    "mpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(mpf):\n",
    "    '''获取 MPF 的特征矩阵'''\n",
    "    return mpf.features[:]\n",
    "\n",
    "\n",
    "def get_labels(mpf):\n",
    "    '''获取 MPF 的标签数组'''\n",
    "    labels_str = mpf.labels.read().decode()\n",
    "    return np.array(labels_str.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = get_features(mpf)  # 获取特征矩阵\n",
    "labels = get_labels(mpf)   # 获取标签\n",
    "h5.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MPF 迭代器\n",
    "\n",
    "依据特征矩阵与标签函数，定义了 MPF 迭代器，获取方式："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CASIAFeature:\n",
    "    def __init__(self, hdf_path):\n",
    "        '''casia 数据 MPF 特征处理工具'''\n",
    "        self.h5 = tb.open_file(hdf_path)\n",
    "\n",
    "    def _features(self, mpf):\n",
    "        '''获取 MPF 的特征矩阵'''\n",
    "        return mpf.features[:]\n",
    "\n",
    "    def _labels(self, mpf):\n",
    "        '''获取 MPF 的标签数组'''\n",
    "        labels_str = mpf.labels.read().decode()\n",
    "        return np.array(labels_str.split(' '))\n",
    "\n",
    "    def __iter__(self):\n",
    "        '''返回 (features, labels)'''\n",
    "        for mpf in self.h5.iter_nodes('/'):\n",
    "            yield self._features(mpf), self._labels(mpf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MPF 迭代器的使用方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3728, 512) (3728,)\n"
     ]
    }
   ],
   "source": [
    "mpf_iter = CASIAFeature(hdf_path)\n",
    "# 以迭代器的方式获取数据\n",
    "for features, labels in mpf_iter:\n",
    "    print(features.shape, labels.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<closed File>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 为了将 CASIA 划分为训练集与测试集，需要重新打包"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重启 Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../loader\")\n",
    "\n",
    "from casia.feature import CASIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CASI 数据集所在根目录\n",
    "root = 'D:/datasets/OCR/CASIA/data'\n",
    "save_path = 'data/features.h5'\n",
    "\n",
    "self = CASIA(root)  # 该类实现数据集的划分\n",
    "self.bunch2hdf(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.98 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 载入 HDF5\n",
    "import tables as tb\n",
    "h5 = tb.open_file(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "/ (RootGroup) \"Xinet's casia dataset\"\n",
       "  children := ['test' (Group), 'train' (Group)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h5.root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3726, 512) (3726,)\n"
     ]
    }
   ],
   "source": [
    "from casia.feature import CASIAFeature\n",
    "\n",
    "mpf_dataset = CASIAFeature(save_path)\n",
    "# 以测试集的迭代器的方式获取数据\n",
    "for features, labels in mpf_dataset.test_iter():\n",
    "    print(features.shape, labels.shape)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
