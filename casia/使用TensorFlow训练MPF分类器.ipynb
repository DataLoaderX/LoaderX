{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../loader\")\n",
    "# 载入 CASIA 的 MPF \n",
    "from casia.feature import CASIAFeature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入一些必备包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras import Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "载入 HDF5 数据并将其转换为训练集与测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf_path = 'data/features.h5'\n",
    "mpf_dataset = CASIAFeature(hdf_path)\n",
    "# 划分数据集为训练集与测试集\n",
    "train_features = np.concatenate([features for features, _ in mpf_dataset.train_iter()])\n",
    "train_labels = np.concatenate([labels for _, labels in mpf_dataset.train_iter()])\n",
    "test_features = np.concatenate([features for features, _ in mpf_dataset.test_iter()])\n",
    "test_labels = np.concatenate([labels for _, labels in mpf_dataset.test_iter()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "分别获取测试集与训练集的样本数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1072276, 4299331)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_labels), len(train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 获取数据的标签及其类别个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集与测试集的类别标签是相同的\n"
     ]
    }
   ],
   "source": [
    "# 获取训练集的类别名称集\n",
    "train_class_names = set(train_labels)\n",
    "# 获取训练测试集的类别名称集\n",
    "test_class_names = set(test_labels)\n",
    "is_same_class = \"相同\" if train_class_names == test_class_names else \"不相同\"\n",
    "print(f'训练集与测试集的类别标签是{is_same_class}的')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于训练集与测试集的类别标签是相同的，所以下面可以将类别名称写作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别个数： 3755\n"
     ]
    }
   ],
   "source": [
    "class_names = test_class_names\n",
    "# 获取类别个数\n",
    "CLASS_NUM = len(class_names)\n",
    "print('类别个数：', CLASS_NUM )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将类别名称转换为 编号 -> 类别名称 的映射关系"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'瘤'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_dict = dict(enumerate(class_names))\n",
    "cat_dict[7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 选择指定类别名称列表的数据集迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from casia.feature import SubCASIA\n",
    "\n",
    "class_names = ['登', '印', '枕', '孤', '美', '好', '琴', '驱', '吞', '山']\n",
    "mpf_dataset = SubCASIA(class_names, hdf_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计有多少个 MPF 文件（即有多少人）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1152"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_train = 0\n",
    "for features, labels in mpf_dataset.sub_train_iter():\n",
    "    n_train += 1\n",
    "    \n",
    "n_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "统计有多少个样本："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的样本数 11500，测试集的样本数 2869\n"
     ]
    }
   ],
   "source": [
    "n_train = 0\n",
    "for features, labels in mpf_dataset.sub_train_iter():\n",
    "    n_train += len(labels) \n",
    "n_test = 0\n",
    "for features, labels in mpf_dataset.sub_test_iter():\n",
    "    n_test += len(labels)\n",
    "print(f\"训练集的样本数 {n_train}，测试集的样本数 {n_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重置 cat_dict："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dict = dict(enumerate(class_names))\n",
    "# 重新划分数据集为训练集与测试集\n",
    "train_features = np.concatenate([features for features, _ in mpf_dataset.sub_train_iter()])\n",
    "train_labels = np.concatenate([labels for _, labels in mpf_dataset.sub_train_iter()])\n",
    "test_features = np.concatenate([features for features, _ in mpf_dataset.sub_test_iter()])\n",
    "test_labels = np.concatenate([labels for _, labels in mpf_dataset.sub_test_iter()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '登',\n",
       " 1: '印',\n",
       " 2: '枕',\n",
       " 3: '孤',\n",
       " 4: '美',\n",
       " 5: '好',\n",
       " 6: '琴',\n",
       " 7: '驱',\n",
       " 8: '吞',\n",
       " 9: '山'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 查看类别字典\n",
    "cat_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将标签数组转换为数值型数组："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取 index -> name 的 dict\n",
    "name2index = {cat:cat_id for cat_id, cat in cat_dict.items()}\n",
    "# 转换训练集的标签\n",
    "train_labels = np.array([name2index[cat_name] for cat_name in train_labels])\n",
    "# 转换测试集的标签\n",
    "test_labels = np.array([name2index[cat_name] for cat_name in test_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将 NumPy 数组转换为 TensorFlow 的迭代器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_features, train_labels))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_features, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将数据分成批量，且打乱训练集数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "SHUFFLE_BUFFER_SIZE = 100\n",
    "\n",
    "train_dataset = train_dataset.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义损失函数，评估函数，优化方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失函数\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "# 定义模型训练的优化方法\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "# 选择衡量指标来度量模型的损失值（loss）和准确率（accuracy）。这些指标在 epoch 上累积值，然后打印出整体结果\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')\n",
    "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
    "test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 `tf.GradientTape` 定义如何训练模型以及评估模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(images, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(images)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_accuracy(labels, predictions)\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def test_step(images, labels):\n",
    "    predictions = model(images)\n",
    "    t_loss = loss_object(labels, predictions)\n",
    "\n",
    "    test_loss(t_loss)\n",
    "    test_accuracy(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self, CLASS_NUM):\n",
    "        super().__init__()\n",
    "        self.d1 = Dense(300, activation='relu')\n",
    "        self.d2 = Dense(CLASS_NUM, activation='softmax')\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.d1(x)\n",
    "        return self.d2(x)\n",
    "\n",
    "\n",
    "# Create an instance of the model\n",
    "CLASS_NUM = len(cat_dict)\n",
    "model = MyModel(CLASS_NUM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练并评估模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.7907116413116455, Accuracy: 92.72173309326172, Test Loss: 3.39178204536438, Test Accuracy: 73.16138458251953\n",
      "Epoch 2, Loss: 0.19209426641464233, Accuracy: 97.57391357421875, Test Loss: 0.3020035922527313, Test Accuracy: 97.00244140625\n",
      "Epoch 3, Loss: 0.0213465616106987, Accuracy: 99.50434875488281, Test Loss: 0.2026280015707016, Test Accuracy: 98.22237396240234\n",
      "Epoch 4, Loss: 0.026385407894849777, Accuracy: 99.80000305175781, Test Loss: 0.20704278349876404, Test Accuracy: 97.94353485107422\n",
      "Epoch 5, Loss: 0.030517058447003365, Accuracy: 99.6956558227539, Test Loss: 0.22136270999908447, Test Accuracy: 97.62983703613281\n",
      "Epoch 6, Loss: 0.012687478214502335, Accuracy: 99.78260803222656, Test Loss: 0.17703032493591309, Test Accuracy: 98.1875228881836\n",
      "Epoch 7, Loss: 0.012077918276190758, Accuracy: 99.77391052246094, Test Loss: 0.15253344178199768, Test Accuracy: 98.25723266601562\n",
      "Epoch 8, Loss: 0.0050966450944542885, Accuracy: 99.87826538085938, Test Loss: 0.19467023015022278, Test Accuracy: 97.90868377685547\n",
      "Epoch 9, Loss: 0.018072351813316345, Accuracy: 99.73912811279297, Test Loss: 0.24186715483665466, Test Accuracy: 97.3161392211914\n",
      "Epoch 10, Loss: 0.05136227235198021, Accuracy: 99.4869613647461, Test Loss: 0.3317306637763977, Test Accuracy: 96.89787292480469\n",
      "Epoch 11, Loss: 0.015679938718676567, Accuracy: 99.6608657836914, Test Loss: 0.22767557203769684, Test Accuracy: 98.39665222167969\n",
      "Epoch 12, Loss: 0.008140776306390762, Accuracy: 99.81739044189453, Test Loss: 0.40621790289878845, Test Accuracy: 96.86302185058594\n",
      "Epoch 13, Loss: 0.017583249136805534, Accuracy: 99.6956558227539, Test Loss: 0.19608813524246216, Test Accuracy: 98.39665222167969\n",
      "Epoch 14, Loss: 0.0193057619035244, Accuracy: 99.73043060302734, Test Loss: 0.1769879162311554, Test Accuracy: 98.501220703125\n",
      "Epoch 15, Loss: 0.0390244759619236, Accuracy: 99.53913116455078, Test Loss: 0.2511923313140869, Test Accuracy: 98.60578918457031\n",
      "Epoch 16, Loss: 0.024286052212119102, Accuracy: 99.66956329345703, Test Loss: 0.3996727764606476, Test Accuracy: 98.04810333251953\n",
      "Epoch 17, Loss: 0.023954492062330246, Accuracy: 99.72174072265625, Test Loss: 0.7149683833122253, Test Accuracy: 95.57337188720703\n",
      "Epoch 18, Loss: 0.023099521175026894, Accuracy: 99.73912811279297, Test Loss: 0.24501514434814453, Test Accuracy: 98.74520874023438\n",
      "Epoch 19, Loss: 0.01940397173166275, Accuracy: 99.75652313232422, Test Loss: 0.29953914880752563, Test Accuracy: 98.15266418457031\n",
      "Epoch 20, Loss: 0.023327859118580818, Accuracy: 99.73043060302734, Test Loss: 0.5314257740974426, Test Accuracy: 96.86302185058594\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for images, labels in train_dataset:\n",
    "        train_step(images, labels)\n",
    "\n",
    "    for test_images, test_labels in test_dataset:\n",
    "        test_step(test_images, test_labels)\n",
    "\n",
    "    template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
    "    print(template.format(epoch+1,\n",
    "                          train_loss.result(),\n",
    "                          train_accuracy.result()*100,\n",
    "                          test_loss.result(),\n",
    "                          test_accuracy.result()*100))\n",
    "\n",
    "    # Reset the metrics for the next epoch\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "    test_loss.reset_states()\n",
    "    test_accuracy.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
